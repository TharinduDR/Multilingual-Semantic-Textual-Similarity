{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Multilingual Semantic Textual Similarity","text":"<p>Semantic textual similarity (STS) is a natural language processing (NLP) task which quantitatively assesses the semantic similarity between two text snippets. STS is a fundamental NLP task for many text-related applications, including text de-duplication, paraphrase detection, semantic searching, and question answering. Measuring STS is a machine learning (ML) problem, where an ML model predicts a value that represents the similarity of the two input texts.</p> <p>This project catalogues datasets annotated for semantic textual similarity. Then we present a comparative evaluation of several deep learning based STS methods in these datasets. In this site, we present the datasets and pre-trained models evaluated in Multilingual Semantic Textual Similarity project.</p> <p>This project is joint work with Dr Tharindu Ranasinghe from the University of Wolverhampton, Prof Ruslan Mitkov from the University of Wolverhampton and Prof Constantin Or\u0103san from the University of Surrey.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>We would like to acknowledge following researchers who helped us with the error analysis.</p> <p>Arabic 1. Mahmoud Gaber 2. Sandra Elfiky 3. Souhila Djabri</p> <p>Brazilian Portuguese 1. Leonardo Foschiera 2. Nath\u00e1lia Marcon</p> <p>French&gt; 1. Ladreyt Alexis 2. Marie Escribe</p> <p>Portuguese&gt; 1. Anna Beatriz Dimas Furtado 2. Patr\u00edcia Freitag</p> <p>Romanian 1. Verginica Barbu Mititelu</p> <p>Serbian 1. Milica Ikoni\u0107 2. Teodora Mihajlov</p> <p>Spanish 1. Ana Isabel Cespedosa 2. Maria Ferragud Ferragud 3. Isabel Pe\u00f1uelas Gil 4. Roc\u00edo Caro Quintana</p> <p>Sinhala 1. Isuri Anuradha 2. Damith Premasiri 3. Kanishka Silva</p> <p>Tamil 1. Prathieshna Vekneswaran 2. Archchana Kugathasan 3. Guruparan Giritharan</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use these resources, please cite (and read!) our paper:</p> <pre><code>Coming soon\n</code></pre>"}]}